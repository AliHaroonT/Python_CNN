{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+2j4OqOLFPLUkstq3dhDb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShoaibSajid/Python_CNN/blob/main/Python_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cw04b-NnPF_e",
        "outputId": "b7e702a7-a6c2-40eb-faf8-5f3880942bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "Github Repo DOES NOT exists.\n",
            "Cloning into 'Python_CNN'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 66 (delta 1), reused 5 (delta 0), pack-reused 60\u001b[K\n",
            "Unpacking objects: 100% (66/66), done.\n",
            "Checking out files: 100% (57/57), done.\n",
            "Github Repo exist.\n",
            "/root/Python_CNN\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mnist==0.2.2\n",
            "  Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting numpy==1.16.3\n",
            "  Downloading numpy-1.16.3-cp37-cp37m-manylinux1_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 422 kB/s \n",
            "\u001b[?25hCollecting keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 20.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4->-r requirements.txt (line 3)) (3.1.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4->-r requirements.txt (line 3)) (1.7.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4->-r requirements.txt (line 3)) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4->-r requirements.txt (line 3)) (6.0)\n",
            "Collecting scipy>=0.14\n",
            "  Downloading scipy-1.7.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.2 MB 1.4 MB/s \n",
            "\u001b[?25h  Downloading scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 76 kB/s \n",
            "\u001b[?25h  Downloading scipy-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 1.9 MB/s \n",
            "\u001b[?25h  Downloading scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4 MB 1.9 MB/s \n",
            "\u001b[?25h  Downloading scipy-1.6.2-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4 MB 47.8 MB/s \n",
            "\u001b[?25h  Downloading scipy-1.6.1-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4 MB 1.6 MB/s \n",
            "\u001b[?25h  Downloading scipy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4 MB 1.6 MB/s \n",
            "\u001b[?25h  Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 61.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.4->-r requirements.txt (line 3)) (1.5.2)\n",
            "Installing collected packages: numpy, scipy, keras-applications, mnist, keras\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.16.3 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.16.3 which is incompatible.\n",
            "tensorflow 2.9.2 requires keras<2.10.0,>=2.9.0rc0, but you have keras 2.2.4 which is incompatible.\n",
            "tensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.16.3 which is incompatible.\n",
            "scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.16.3 which is incompatible.\n",
            "resampy 0.4.2 requires numpy>=1.17, but you have numpy 1.16.3 which is incompatible.\n",
            "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.16.3 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.16.3 which is incompatible.\n",
            "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.16.3 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.16.3 which is incompatible.\n",
            "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.16.3 which is incompatible.\n",
            "numba 0.56.3 requires numpy<1.24,>=1.18, but you have numpy 1.16.3 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.16.3 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "jax 0.3.23 requires numpy>=1.20, but you have numpy 1.16.3 which is incompatible.\n",
            "gym 0.25.2 requires numpy>=1.18.0, but you have numpy 1.16.3 which is incompatible.\n",
            "cmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.16.3 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.16.3 which is incompatible.\n",
            "aesara 2.7.9 requires numpy>=1.17.0, but you have numpy 1.16.3 which is incompatible.\n",
            "aeppl 0.0.33 requires numpy>=1.18.1, but you have numpy 1.16.3 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.2.4 keras-applications-1.0.8 mnist-0.2.2 numpy-1.16.3 scipy-1.5.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "List of files\n",
            "cnn_keras.py\t     conv.py\t  linear_biases.npy   relu.py\t\tweights\n",
            "cnn.py\t\t     data.csv\t  linear_filters.npy  requirements.txt\n",
            "cnn_torch.py\t     fc.py\t  maxpool.py\t      soft_fc.py\n",
            "conv_filters_3d.npy  filters.npy  __pycache__\t      softmax.py\n",
            "conv_filters.npy     LICENSE\t  README.md\t      softmax_test.py\n"
          ]
        }
      ],
      "source": [
        "%cd ~\n",
        "![ ! -d \"./Python_CNN\" ] && echo \"Github Repo DOES NOT exists.\"\n",
        "![ ! -d \"./Python_CNN\" ] && git clone https://github.com/ShoaibSajid/Python_CNN\n",
        "![ -d \"./Python_CNN\" ] && echo \"Github Repo exist.\"\n",
        "%cd Python_CNN\n",
        "%pip install -r requirements.txt\n",
        "!echo \n",
        "!echo List of files\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "\n",
        "from logging import raiseExceptions\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import mnist\n",
        "import numpy as np\n",
        "from conv import Conv3x3, Conv3x3_n_to_n_padding, Conv3x3_1_to_n_padding\n",
        "from maxpool import MaxPool2\n",
        "from softmax import Softmax\n",
        "from relu import Relu\n",
        "from softmax_test import Softmax_test\n",
        "from fc import FC"
      ],
      "metadata": {
        "id": "I-725DWZTpo6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network layers\n",
        "\n",
        "conv0   = Conv3x3_1_to_n_padding( output=8                        )     # 28x28x1   -> 28x28x8  (Convolution with 8 filters)\n",
        "pool0   = MaxPool2              (                                 )     # 28x28x8   -> 14x14x8  (MaxPooling 2x2)\n",
        "conv1   = Conv3x3_n_to_n_padding( output=16     ,   input=8       )     # 14x14x8   -> 14x14x16 (Convolution with 8 filters)\n",
        "pool1   = MaxPool2              (                                 )     # 14x14x16  -> 07x07x16 (MaxPooling 2x2)\n",
        "# conv2   = Conv3x3_n_to_n_padding( output=32     ,   input=16      )\n",
        "# conv3   = Conv3x3_n_to_n_padding( output=64     ,   input=32      )\n",
        "fc0     = FC                    ( 7 * 7 * 16  ,   7 * 7 * 16      )     # 784       -> 784      (FC)\n",
        "fc1     = FC                    ( 7 * 7 * 16  ,   10              )     # 784       -> 10       (FC)\n",
        "softmax = Softmax               (                                 )     # 13x13x8   -> 10       (Softmax)\n",
        "relu    = Relu                  (                                 )     # 13x13x8   -> 10       (Softmax)"
      ],
      "metadata": {
        "id": "iAoYBSKmWKWE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the network - Forward propagation\n",
        "def forward(im, label, debug=False):\n",
        "  im        = (im / 255) - 0.5  \n",
        "    \n",
        "  # Conv 0 with Pool\n",
        "  out_conv0 = conv0.forward   ( im            )\n",
        "  out_pool0 = pool0.forward   ( out_conv0     )\n",
        "  \n",
        "  # Conv 1 with Pool\n",
        "  out_conv1 = conv1.forward   ( out_pool0     )\n",
        "  out_pool1 = pool1.forward   ( out_conv1     )\n",
        "  \n",
        "  # Swap axes to realign for flattening\n",
        "  out_pool2 = np.swapaxes(out_pool1,0,2)\n",
        "  out_pool3 = np.swapaxes(out_pool2,1,2)\n",
        "  \n",
        "  # FC0 and Relu\n",
        "  out_fc0   = fc0.forward     ( out_pool3     )\n",
        "  \n",
        "  # FC1 and SoftMax\n",
        "  out_fc1   = fc1.forward     ( out_fc0       )\n",
        "  out_soft  = softmax.forward ( out_fc1       )\n",
        "  \n",
        "  if debug:\n",
        "    print(f\"Input Image: {im[-4]}\\n\")\n",
        "    print(f\"x_conv0 filters : {conv0.filters[0]}\\n\")\n",
        "    print(f\"x_conv0 : {out_conv0[:,:,0][-1]}\\n\")\n",
        "    print(f\"MaxPool0: {out_pool0[:,:,0][-1]}\\n\")\n",
        "    print(f\"x_conv1 filters : {conv1.filters[0,:,:,0]}\\n\")\n",
        "    print(f\"x_conv1 : {out_conv1[:,:,0][-1]}\\n\")\n",
        "    print(f\"MaxPool1: {out_pool1[:,:,0][-1]}\\n\")\n",
        "    print(f\"FC0 Weights: {fc0.weights[:,0][:10]}\\n\")\n",
        "    print(f\"FC0 output: {out_fc0[:10]}\\n\")\n",
        "    print(f\"FC1 Weights: {fc1.weights[:,0][:10]}\\n\")\n",
        "    print(f\"FC1 output: {out_fc1[:10]}\\n\")\n",
        "    print(f\"SoftMax output: {out_soft}\\n\")\n",
        "\n",
        "  return out_soft\n"
      ],
      "metadata": {
        "id": "EwHWdHFrWSao"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the network - Backward propagation\n",
        "def backward(label, out, lr=0.005):\n",
        "  # Calculate initial gradient\n",
        "  gradient = np.zeros(10)\n",
        "  gradient[label] = -1 / out[label]\n",
        "\n",
        "  # ------------------------------Backprop-----------------------------------\n",
        "  # SoftMax\n",
        "  gradient_softmax = softmax.backprop ( gradient                              )     \n",
        "\n",
        "  # FC1 and FC0 \n",
        "  gradient_fc1 = fc1.backprop         ( gradient_softmax  ,               lr  )\n",
        "  gradient_fc0 = fc0.backprop         ( gradient_fc1      ,               lr  )\n",
        "\n",
        "  # Swap axes to realign for flattening\n",
        "  gradient_swap0 = np.swapaxes        ( gradient_fc0      ,       1   ,   2   )\n",
        "  gradient_swap1 = np.swapaxes        ( gradient_swap0    ,       0   ,   2   )\n",
        "\n",
        "  # Conv 1 with Pool \n",
        "  gradient_pool1 = pool1.backprop     ( gradient_swap1                        )\n",
        "  gradient_conv1 = conv1.backprop     ( gradient_pool1     ,              lr  )\n",
        "\n",
        "  # Conv 0 with Pool \n",
        "  gradient_pool0 = pool0.backprop     ( gradient_conv1                        ) \n",
        "  gradient_conv0 = conv0.backprop     ( gradient_pool0     ,              lr  )\n",
        "  return None"
      ],
      "metadata": {
        "id": "IoomUOeIWg1h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the network - Loss Function (Cross Entropy)\n",
        "# Calculate cross-entropy loss and accuracy. np.log() is the natural log.\n",
        "def cal_loss(out_soft, label):\n",
        "  loss = -np.log(out_soft[label])\n",
        "  acc = 1 if np.argmax(out_soft) == label else 0\n",
        "  return out_soft, loss, acc"
      ],
      "metadata": {
        "id": "yhMPrtRNXBXT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the network - Training Function\n",
        "def train(im, label, debug=False, lr=.005):\n",
        "  pred = forward(im, label, debug)\n",
        "  out_soft, loss, acc = cal_loss(pred, label)\n",
        "  backward(label, out_soft, lr=0.005)\n",
        "  return loss, acc"
      ],
      "metadata": {
        "id": "jbypb9x2XRej"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the network - Validation Function\n",
        "def val(im, label):\n",
        "  pred = forward(im, label)\n",
        "  out_soft, loss, acc = cal_loss(pred, label)\n",
        "  return loss, acc"
      ],
      "metadata": {
        "id": "Qlrl0fzUXV_M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function definitions to save and load weight files\n",
        "\n",
        "def save_weights(name,lr=0,max_acc=0):\n",
        "  print(f\"\\nSaving new weights ({name}).\")\n",
        "  weights = dict()\n",
        "  weights[\"conv0\"]        = conv0.filters\n",
        "  weights[\"conv1\"]        = conv1.filters\n",
        "  weights[\"fc0_weights\"]  = fc0.weights\n",
        "  weights[\"fc0_biases\" ]  = fc0.biases\n",
        "  weights[\"fc1_weights\"]  = fc1.weights\n",
        "  weights[\"fc1_biases\" ]  = fc1.biases\n",
        "  weights[\"lr\" ]          = lr\n",
        "  weights[\"max_acc\"]      = max_acc\n",
        "  weight_file = open(str(name), \"wb\")\n",
        "  pickle.dump(weights, weight_file)\n",
        "  weight_file.close()\n",
        "  \n",
        "def load_weights(name):\n",
        "  if os.path.isfile(name): \n",
        "    weight_file = open(str(name), \"rb\")\n",
        "    weights = pickle.load(weight_file)\n",
        "    conv0.filters  = weights[\"conv0\"]      \n",
        "    conv1.filters  = weights[\"conv1\"]      \n",
        "    fc0.weights    = weights[\"fc0_weights\"]\n",
        "    fc0.biases     = weights[\"fc0_biases\" ]\n",
        "    fc1.weights    = weights[\"fc1_weights\"]\n",
        "    fc1.biases     = weights[\"fc1_biases\" ]\n",
        "    lr             = weights[\"lr\" ]\n",
        "    max_acc        = weights[\"max_acc\"]\n",
        "    print(f\"\\nLoading weights from {name} file. LR restored to {lr}. Last Accuracy {max_acc}%\")\n",
        "    return lr, max_acc\n",
        "  else:\n",
        "    print(\"Weights file not found.\")\n",
        "    lr=0.005\n",
        "    max_acc=0\n",
        "    return lr, max_acc\n",
        "   "
      ],
      "metadata": {
        "id": "n3132AoGXzc0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the network - Adjust learning rate\n",
        "def adjust_lr(acc, lr=.005):\n",
        "  if   acc > 98: lr=0.00001\n",
        "  elif acc > 95: lr=0.0005\n",
        "  elif acc > 90: lr=0.001\n",
        "  elif acc > 80: lr=0.002\n",
        "  elif acc > 70: lr=0.003\n",
        "  elif acc > 60: lr=0.004\n",
        "  return lr"
      ],
      "metadata": {
        "id": "Y1WIfF2UYstz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define settings for run\n",
        "\n",
        "debug=False\n",
        "\n",
        "shuffle_data = False\n",
        "\n",
        "run_train = True\n",
        "run_val   = True\n",
        "\n",
        "load_saved_weights = True\n",
        "weight_file = 'weights/best_99.pkl'\n",
        "\n",
        "total_epoch = 10\n",
        "training_acc_internal = 1000"
      ],
      "metadata": {
        "id": "oidgGH5PVVWS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Weights\n",
        "if load_saved_weights:\n",
        "    lr, max_acc = load_weights(weight_file)\n",
        "else:\n",
        "    lr, max_acc = 0.005, 0 \n",
        "\n",
        "if debug: save_weights(f'weights/debug.pkl', lr, max_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RYPUzZDYHcC",
        "outputId": "e1597a93-d0fa-4150-b317-0d625f7a374b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading weights from weights/best_99.pkl file. LR restored to 0.0005. Last Accuracy 99%\n",
            "\n",
            "Saving new weights (weights/debug.pkl).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the training dataset - MNIST Dataset\n",
        "\n",
        "train_images = mnist.train_images()\n",
        "train_labels = mnist.train_labels()\n",
        "test_images = mnist.test_images()\n",
        "test_labels = mnist.test_labels()"
      ],
      "metadata": {
        "id": "0SG6nSKmXoi7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle Data\n",
        "permutation = np.random.permutation(len(train_images))\n",
        "train_images = train_images[permutation]\n",
        "train_labels = train_labels[permutation]"
      ],
      "metadata": {
        "id": "4wNl_3XuYbOB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Main function to run the training\n",
        "\n",
        "if run_train:\n",
        "  print(f'Training Initialized.')\n",
        "  print(f\"\\tTotal number of training   images: {len(train_labels)}\")\n",
        "  print(f\"\\tTotal number of validation images: {len(test_labels)}\")\n",
        "  print(f\"\\tTraining will run for {total_epoch} epochs.\")\n",
        "  print(f\"\\tResults will be logged after every {training_acc_internal} images.\")\n",
        "  for epoch in range(total_epoch):\n",
        "    print('\\n--- Epoch %d ---' % (epoch + 1))\n",
        "            \n",
        "    # Initialize Variables\n",
        "    loss, num_correct = 0, 0\n",
        "    for i, (im, label) in tqdm(enumerate(zip(train_images, train_labels))):\n",
        "      \n",
        "      # Logging results\n",
        "      if i % training_acc_internal == training_acc_internal-1:\n",
        "        lr = adjust_lr(num_correct)\n",
        "        if num_correct > max_acc: \n",
        "          max_acc = num_correct\n",
        "          save_weights(f'weights/best_{num_correct}.pkl', lr, max_acc)\n",
        "          save_weights(f'weights/last.pkl', lr, max_acc)\n",
        "        print(f'\\n[Step {(i+1)}] : Avg Loss for {training_acc_internal} iterations is {np.round((loss / 100),2)} | Training Acc: {num_correct} | LR: {lr}')\n",
        "        loss = 0\n",
        "        num_correct = 0\n",
        "          \n",
        "      # Train the network\n",
        "      l, acc = train(im, label, debug, lr=lr)\n",
        "      loss += l\n",
        "      num_correct += acc\n",
        "           \n",
        "    print(f\"End of epoch {epoch+1}\")      \n",
        "\n",
        "    print(f\"\\n\\nCalculating validation scores at the end of epoch.\")\n",
        "    loss, num_correct = 0, 0\n",
        "    for im, label in tqdm(zip(test_images, test_labels)):\n",
        "      l, acc = val(im, label)\n",
        "      loss += l\n",
        "      num_correct += acc\n",
        "    num_tests = len(test_images)\n",
        "    print('Test Loss:', loss / num_tests)\n",
        "    print('Test Accuracy:', num_correct / num_tests)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N02tnG1ZVSyk",
        "outputId": "f144dc80-395e-4832-884f-f117b2e5e33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Initialized.\n",
            "\tTotal number of training   images: 60000\n",
            "\tTotal number of validation images: 10000\n",
            "\tTraining will run for 10 epochs.\n",
            "\tResults will be logged after every 1000 images.\n",
            "\n",
            "--- Epoch 1 ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [03:24,  5.15it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving new weights (weights/best_938.pkl).\n",
            "\n",
            "Saving new weights (weights/last.pkl).\n",
            "\n",
            "[Step 1000] : Avg Loss for 1000 iterations is 2.23 | Training Acc: 938 | LR: 1e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2000it [06:48,  5.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Step 2000] : Avg Loss for 1000 iterations is 2.05 | Training Acc: 929 | LR: 1e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2070it [07:02,  5.22it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to validate the weights\n",
        "\n",
        "if run_val:\n",
        "  print(f'\\n--- Testing the CNN for {len(test_labels)} images---')\n",
        "  loss = 0\n",
        "  num_correct = 0\n",
        "  for im, label in tqdm(zip(test_images, test_labels)):\n",
        "    l, acc = val(im, label)\n",
        "    loss += l\n",
        "    num_correct += acc\n",
        "\n",
        "  num_tests = len(test_images)\n",
        "  print('\\nTest Loss:', loss / num_tests)\n",
        "  print('Test Accuracy:', num_correct / num_tests)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "PonPg6kBZT1J",
        "outputId": "083c6df2-76a3-402c-a5db-b8397ec77c5c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing the CNN for 10000 images---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "73it [00:05, 12.99it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b5d1b950e379>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-6db71294e5c5>\u001b[0m in \u001b[0;36mval\u001b[0;34m(im, label)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Defining the network - Validation Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mout_soft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-9474bf395226>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(im, label, debug)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Conv 1 with Pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mout_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m   \u001b[0;34m(\u001b[0m \u001b[0mout_pool0\u001b[0m     \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mout_pool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m   \u001b[0;34m(\u001b[0m \u001b[0mout_conv1\u001b[0m     \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Python_CNN/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mim_region\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_regions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_filters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_region\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2260\u001b[0;31m         \u001b[0mInput\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2261\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m         \u001b[0mAxis\u001b[0m \u001b[0malong\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcumulative\u001b[0m \u001b[0msum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcomputed\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}